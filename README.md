# ğŸ“Š Data Pipeline - Receita Federal de Empresas

Pipeline de dados para processamento de informaÃ§Ãµes cadastrais de empresas brasileiras da Receita Federal.

## ğŸ—ï¸ Arquitetura

``` sh
graph TB
    A[Receita Federal<br>Dados Abertos] --> B[Bronze Layer<br>Raw Data]
    B --> C[Silver Layer<br>Cleaned Data]
    C --> D[Gold Layer<br>Business Data]
    D --> E[Superset<br>Dashboards]
    
    subgraph "OrquestraÃ§Ã£o"
        F[Airflow DAG]
    end
    
    subgraph "Data Quality"
        G[Pandera Validation]
        H[Quality Metrics]
    end
    
    F --> B
    F --> C
    F --> D
    G --> C
    G --> D
    H --> E
```

## ğŸ“‹ Funcionalidades

- âœ… IngestÃ£o de dados dos arquivos ZIP da Receita Federal
- âœ… Processamento em camadas (Bronze â†’ Silver â†’ Gold)
- âœ… ValidaÃ§Ã£o de dados com Pandera
- âœ… Monitoramento de qualidade com mÃ©tricas em PostgreSQL
- âœ… Dashboards no Apache Superset
- âœ… OrquestraÃ§Ã£o com Apache Airflow

## ğŸ› ï¸ Tecnologias
- Python 3.12 - Processamento de dados
- Pandas - ManipulaÃ§Ã£o de dados
- Airflow - OrquestraÃ§Ã£o de pipelines
- Pandera - ValidaÃ§Ã£o de dados
- PostgreSQL - Armazenamento de dados e mÃ©tricas
- Superset - VisualizaÃ§Ã£o de dados
- Docker - ContainerizaÃ§Ã£o

## ğŸ“ Estrutura do Projeto

``` sh
receita-federal-empresas/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ main_pipeline.py          # DAG principal do Airflow
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_ingestion/           # IngestÃ£o de dados brutos
â”‚   â”‚   â”œâ”€â”€ companies.py
â”‚   â”‚   â””â”€â”€ partners.py
â”‚   â”œâ”€â”€ data_processing/          # Processamento das camadas
â”‚   â”‚   â”œâ”€â”€ companies.py
â”‚   â”‚   â””â”€â”€ partners.py
â”‚   â”œâ”€â”€ data_refinement/          # Camada gold
â”‚   â”‚   â””â”€â”€ companies_detail.py
â”‚   â””â”€â”€ data_quality/             # ValidaÃ§Ãµes com Pandera
â”‚       â”œâ”€â”€ bronze_validation.py
â”‚       â”œâ”€â”€ silver_validation.py
â”‚       â””â”€â”€ gold_validation.py
â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ Dockerfile                    # Imagem customizada
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â””â”€â”€ .env                          # VariÃ¡veis de ambiente
```

## ğŸš€ Setup do Ambiente
### PrÃ©-requisitos
- Docker 20.10+
- Docker Compose 2.0+
- 8GB RAM recomendado

### 1. Clone o repositÃ³rio
``` sh
git clone https://github.com/seu-usuario/receita-federal-empresas.git
cd receita-federal-empresas
```
### 2. Configure as variÃ¡veis de ambiente
``` sh
cp .env.example .env
# Edite o .env com suas configuraÃ§Ãµes
```
### 3. Execute os containers
``` sh
# Inicie todos os serviÃ§os
docker-compose up -d

# Ou construa as imagens e inicie
docker-compose build
docker-compose up -d
```
### 4. Acesse as interfaces
- Airflow: http://localhost:8080
    - UsuÃ¡rio: airflow
    - Senha: airflow
- Superset: http://localhost:8088
    - UsuÃ¡rio: admin
    - Senha: admin

## ğŸ“Š Camadas de Dados

### ğŸ¥‰ Bronze Layer (Raw)
- Dados brutos ingeridos da Receita Federal
- Formato original preservado
- ValidaÃ§Ãµes bÃ¡sicas de formato

### ğŸ¥ˆ Silver Layer (Cleaned)
- Dados limpos e tratados
- Schema validation com Pandera
- PadronizaÃ§Ã£o de formatos

### ğŸ¥‡ Gold Layer (Business)
- Dados enriquecidos para anÃ¡lise
- MÃ©tricas de negÃ³cio
- AgregaÃ§Ãµes e transformaÃ§Ãµes

## âœ… Data Quality Checks

### ValidaÃ§Ãµes Implementadas
#### Camada Silver:
- âœ… CNPJ com 14 dÃ­gitos
- âœ… CÃ³digo de porte vÃ¡lido (00, 01, 03, 05)
- âœ… Natureza jurÃ­dica dentro da faixa esperada
- âœ… Capital social nÃ£o negativo
#### Camada Gold:
- âœ… Quantidade de sÃ³cios > 0
- âœ… Flags booleanas consistentes
- âœ… Regras de negÃ³cio aplicadas

### MÃ©tricas Monitoradas
``` sql
-- Exemplo de mÃ©tricas coletadas
SELECT * FROM data_quality_metrics 
WHERE table_name = 'companies' 
ORDER BY created_at DESC 
LIMITE 10;
```

## ğŸ“ˆ Dashboards no Superset

TBD

## ğŸ”§ Comandos Ãšteis

### Docker Compose
``` sh
# Ver status dos containers
docker-compose ps

# Logs do Airflow
docker-compose logs airflow-scheduler

# Executar comando em container especÃ­fico
docker-compose exec airflow-scheduler airflow dags list

# Parar todos os serviÃ§os
docker-compose down

# Parar e remover volumes
docker-compose down -v
```
### Airflow
``` sh
# Listar DAGs
docker-compose exec airflow-scheduler airflow dags list

# Trigger manual da DAG
docker-compose exec airflow-scheduler airflow dags trigger main_pipeline

# Ver logs de uma task
docker-compose exec airflow-webserver airflow tasks logs main_pipeline bronze_companies
```
### Desenvolvimento
``` sh
# Instalar dependÃªncias localmente
pip install -r requirements.txt

# Executar script individualmente
python scripts/data_ingestion/companies.py

# Testar validaÃ§Ãµes
python scripts/data_quality/silver_validation.py
```

## ğŸ“ PrÃ³ximos Passos
### Melhorias Futuras
-Adicionar testes unitÃ¡rios
- Implementar alertas de qualidade
- Adicionar mais fontes de dados
- Otimizar performance de queries
- Implementar particionamento de dados
### CustomizaÃ§Ã£o
- Editar scripts/data_processing/ para novas transformaÃ§Ãµes
- Modificar scripts/data_quality/ para novas validaÃ§Ãµes
- Ajustar dags/main_pipeline.py para alterar o fluxo
- Customizar queries no Superset para novos dashboards

## ğŸ¤ ContribuiÃ§Ã£o
1. Fork o projeto
2. Crie uma branch para sua feature (git checkout -b feature/AmazingFeature)
3. Commit suas mudanÃ§as (git commit -m 'Add some AmazingFeature')
4. Push para a branch (git push origin feature/AmazingFeature)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a
Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.